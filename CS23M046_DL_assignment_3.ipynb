{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8316432,"sourceType":"datasetVersion","datasetId":4940077},{"sourceId":8437279,"sourceType":"datasetVersion","datasetId":5025595}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wandb==0.14.0","metadata":{"id":"P9oIymAh7aKG","outputId":"fa0f3f0d-eea0-47e0-a927-30d4a4b4b24d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{"id":"BeSAGQ8PIhdt"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nimport wandb\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nfrom tqdm import tqdm\nfrom IPython.display import display\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"uEqHxH1GE9Eu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoder and Decoder class","metadata":{"id":"wLHD-96_Ik5f"}},{"cell_type":"code","source":"# Define the model architecture\nclass Encoder(nn.Module):\n    \"\"\"\n    Encoder module of the sequence-to-sequence model.\n\n    Parameters:\n        input_size (int): The size of the input vocabulary.\n        embedding_size (int): The dimension of the embedding layer.\n        hidden_size (int): The size of the hidden state of the RNN.\n        batch_size (int): The size of the batches.\n        encoder_num_layers (int): The number of layers in the encoder.\n        cell_type (str): The type of RNN cell: \"RNN\", \"LSTM\", or \"GRU\".\n        bidirectional (bool): Whether to use bidirectional RNN.\n        dropout (float): Dropout rate.\n\n    Inputs:\n        input_seq (tensor): Input sequence tensor.\n        hidden (tensor): Initial hidden state tensor.\n\n    Outputs:\n        outputs (tensor): Encoder outputs tensor.\n        hidden (tensor): Final hidden state tensor.\n    \"\"\"\n\n    def __init__(self, input_size, embedding_size, hidden_size, batch_size, encoder_num_layers, cell_type, bidirectional, dropout):\n        super(Encoder, self).__init__()\n        # declaring class variables\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.dropout = nn.Dropout(dropout)\n        self.batch_size = batch_size\n        self.embedding_size = embedding_size\n        self.encoder_num_layers = encoder_num_layers\n        self.cell_type = cell_type\n        self.bidirectional = bidirectional\n\n\n        # defining the layers\n        if cell_type == \"RNN\":\n            self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.encoder_num_layers, bidirectional=bidirectional, dropout=dropout)\n        elif cell_type == \"LSTM\":\n            self.rnn = nn.LSTM(self.embedding_size, self.hidden_size, self.encoder_num_layers, bidirectional=bidirectional, dropout=dropout)\n        elif cell_type == \"GRU\":\n            self.rnn = nn.GRU(self.embedding_size, self.hidden_size, self.encoder_num_layers, bidirectional=bidirectional, dropout=dropout)\n\n\n    def forward(self, input_seq, hidden):\n        # Embedding layer\n        embedded = self.dropout((self.embedding(input_seq.long())).view(-1, self.batch_size, self.embedding_size))\n        # RNN layer\n        outputs, hidden = self.rnn(embedded, hidden)\n\n\n        # if birection is true\n        if self.bidirectional:\n            # LSTM has 3 outputs so handling them\n            if self.cell_type == \"LSTM\":\n                # Dividing the hidden state into parts for each direction\n                hidden_state = hidden[0].view(2, self.encoder_num_layers, self.batch_size, self.hidden_size)\n                cell_state = hidden[0].view(2, self.encoder_num_layers, self.batch_size, self.hidden_size)\n\n                # Combining the hidden and cell states by taking their average\n                hidden = (torch.add(hidden_state[0], hidden_state[1]) / 2, torch.add(cell_state[0], cell_state[1]) / 2)\n            else:\n                # Dividing the hidden state into parts for each direction\n                hidden = hidden.view(2, self.encoder_num_layers, self.batch_size, self.hidden_size)\n\n                # Combining the hidden states by taking their average\n                hidden = torch.add(hidden[0], hidden[1]) / 2\n\n            # Splitting the output tensor into parts for each direction\n            split_tensor = torch.split(outputs, self.hidden_size, dim=-1)\n\n            # Combining the outputs by taking their average\n            outputs = torch.add(split_tensor[0], split_tensor[1]) / 2\n\n        return outputs, hidden\n\n    def initHidden(self):\n        # Initializing the hidden state\n        num_directions = 2 if self.bidirectional else 1  # For bidirectional, set to 2, otherwise 1\n        if self.cell_type == \"LSTM\":\n            return (torch.zeros(self.encoder_num_layers * num_directions, self.batch_size, self.hidden_size, device=device),\n                    torch.zeros(self.encoder_num_layers * num_directions, self.batch_size, self.hidden_size, device=device))\n        else:\n            return torch.zeros(self.encoder_num_layers * num_directions, self.batch_size, self.hidden_size, device=device)\n\n\nclass Decoder(nn.Module):\n    \"\"\"\n    Decoder module of the sequence-to-sequence model.\n\n    Parameters:\n        output_size (int): The size of the output vocabulary.\n        embedding_size (int): The dimension of the embedding layer.\n        hidden_size (int): The size of the hidden state of the RNN.\n        batch_size (int): The size of the batches.\n        decoder_num_layers (int): The number of layers in the decoder.\n        cell_type (str): The type of RNN cell: \"RNN\", \"LSTM\", or \"GRU\".\n        dropout (float): Dropout rate.\n        MAX_LENGTH (int): Maximum length of the input sequence.\n\n    Inputs:\n        input (tensor): Input tensor.\n        hidden (tensor): Hidden state tensor.\n\n    Outputs:\n        output (tensor): Output tensor.\n        hidden (tensor): Hidden state tensor.\n    \"\"\"\n\n    def __init__(self, output_size, embedding_size, hidden_size, batch_size, decoder_num_layers, cell_type, dropout, MAX_LENGTH):\n        super(Decoder, self).__init__()\n        # declaring class variables\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.batch_size = batch_size\n        self.embedding_size = embedding_size\n        self.decoder_num_layers = decoder_num_layers\n        self.MAX_LENGTH = MAX_LENGTH\n        self.cell_type = cell_type\n        self.dropout = dropout\n\n        # defining the layers\n        self.embedding = nn.Embedding(output_size, embedding_size)\n        self.dropout = nn.Dropout(dropout)\n        if cell_type == \"RNN\":\n            self.rnn = nn.RNN(embedding_size, hidden_size, self.decoder_num_layers, dropout=dropout)\n        elif cell_type == \"LSTM\":\n            self.rnn = nn.LSTM(embedding_size, hidden_size, self.decoder_num_layers, dropout=dropout)\n        elif cell_type == \"GRU\":\n            self.rnn = nn.GRU(embedding_size, hidden_size, self.decoder_num_layers, dropout=dropout)\n\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=2)\n\n    def forward(self, input, hidden):\n        output = self.embedding(input.long()).view(-1, self.batch_size, self.embedding_size)\n        output = F.relu(output)\n        output, hidden = self.rnn(output, hidden)\n        output = self.softmax(self.out(output))\n        return output, hidden\n","metadata":{"id":"0zYZp7DgFMzO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decoder with Attention","metadata":{"id":"cq0FyZHcIpPG"}},{"cell_type":"code","source":"class AttentionDecoderRNN(nn.Module):\n    \"\"\"\n    Attention-based Decoder module of the sequence-to-sequence model.\n\n    Parameters:\n        hidden_size (int): The size of the hidden state of the RNN.\n        output_size (int): The size of the output vocabulary.\n        embedding_size (int): The dimension of the embedding layer.\n        num_layers (int): The number of layers in the RNN.\n        cell_type (str): The type of RNN cell: \"RNN\", \"LSTM\", or \"GRU\".\n        dropout (float): Dropout rate.\n        batch_size (int): The size of the batches.\n        max_length (int): Maximum length of the input sequence.\n\n    Inputs:\n        input (tensor): Input tensor.\n        hidden (tensor): Hidden state tensor.\n        encoder_outputs (tensor): Encoder outputs tensor.\n\n    Outputs:\n        output (tensor): Output tensor.\n        hidden (tensor): Hidden state tensor.\n        attn_weights (tensor): Attention weights tensor.\n    \"\"\"\n\n    def __init__(self, hidden_size, output_size, embedding_size, num_layers,\n                 cell_type, dropout, batch_size, max_length):\n        super(AttentionDecoderRNN, self).__init__()\n        # declaring class variables\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        self.batch_size = batch_size\n        self.embedding_size = embedding_size\n        self.max_length = max_length\n        self.dropout = dropout\n\n        self.embedding = nn.Embedding(output_size, embedding_size)\n        self.dropout = nn.Dropout(self.dropout)\n        self.attention = nn.Linear(hidden_size + embedding_size, self.max_length)\n        self.attention_combine = nn.Linear(hidden_size + embedding_size, hidden_size)\n\n        if self.cell_type == \"GRU\":\n            self.rnn = nn.GRU(hidden_size, hidden_size, num_layers=num_layers)\n        elif self.cell_type == \"LSTM\":\n            self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)\n        else:\n            self.rnn = nn.RNN(hidden_size, hidden_size, num_layers=num_layers)\n\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=2)\n\n    def forward(self, input, hidden, encoder_outputs):\n        # defining embedding\n        embedded = self.embedding(input.long()).view(-1, self.batch_size, self.embedding_size)\n        embedded = F.relu(embedded)\n\n        # Compute attention scores\n        if self.cell_type == \"LSTM\":\n            attn_hidden = torch.mean(hidden[0], dim=0)\n        else:\n            attn_hidden = torch.mean(hidden, dim=0)\n\n        attn_scores = self.attention(torch.cat((embedded, attn_hidden.unsqueeze(0)), dim=2))\n\n        attn_weights = F.softmax(attn_scores, dim=-1)\n\n        # Apply attention weights to encoder outputs\n        attn_applied = torch.bmm(attn_weights.transpose(0, 1), encoder_outputs.transpose(0, 1))\n\n        # Combine attention output and embedded input\n        combined = torch.cat((embedded, attn_applied.transpose(0, 1)), dim=2)\n        combined = self.attention_combine(combined)\n        combined = F.relu(combined)\n\n        # Run through the RNN\n        output, hidden = self.rnn(combined, hidden)\n\n        # Pass through linear layer and softmax activation\n        output = self.out(output)\n        output = self.softmax(output)\n        return output, hidden, attn_weights.transpose(0, 1)\n","metadata":{"id":"EowNTEKgHdOj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Retrival","metadata":{"id":"r0UzzIPwJCRQ"}},{"cell_type":"code","source":"def Datasetretrival(folder_path, folder_name):\n    \"\"\"\n    Retrieve datasets from the specified folder path and folder name.\n\n    Parameters:\n        folder_path (str): The path to the folder containing the datasets.\n        folder_name (str): The name of the folder containing the datasets.\n\n    Returns:\n        pandas.DataFrame: The training dataset.\n        pandas.DataFrame: The validation dataset.\n        pandas.DataFrame: The testing dataset.\n    \"\"\"\n    # Initialize empty lists for train, test, and validation datasets\n    train_datasets = []\n    test_datasets = []\n    val_datasets = []\n\n    # Create the full folder path by joining the provided folder path and folder name\n    folder_path = os.path.join(folder_path, folder_name)\n\n    # List all files in the folder\n    folder_files = os.listdir(folder_path)\n\n    # Filter files with the specified folder name as prefix\n    foldername_prefix = folder_name + \"_\"\n    folder_files_with_prefix = [file for file in folder_files if file.startswith(foldername_prefix)]\n\n    # Loop through the files in the folder\n    for file in folder_files_with_prefix:\n        file_path = os.path.join(folder_path, file)\n        # Check if the file is for training, testing, or validation\n        if 'train' in file:\n            # Append the training data to the train_datasets list\n            train_datasets.append(pd.read_csv(file_path, header=None))\n        elif 'test' in file:\n            # Append the testing data to the test_datasets list\n            test_datasets.append(pd.read_csv(file_path, header=None))\n        elif 'val' in file:\n            # Append the validation data to the val_datasets list\n            val_datasets.append(pd.read_csv(file_path, header=None))\n\n    # Concatenate the loaded dataframes to create single train, test, and validation datasets\n    train_dataset = pd.concat(train_datasets, ignore_index=True)\n    test_dataset = pd.concat(test_datasets, ignore_index=True)\n    val_dataset = pd.concat(val_datasets, ignore_index=True)\n\n    return train_dataset, val_dataset, test_dataset\n","metadata":{"id":"k8rQXUjvHdOk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorization","metadata":{"id":"EV7-uDqLJG8J"}},{"cell_type":"code","source":"def vecorizeddata(data_pairs, index2char, char2index, MAX_LENGTH):\n    \"\"\"\n    Vectorize the input data pairs for training.\n\n    Parameters:\n        data_pairs (list): List of tuples containing pairs of input-output data.\n        index2char (dict): Dictionary mapping indices to characters.\n        char2index (dict): Dictionary mapping characters to indices.\n        MAX_LENGTH (int): Maximum length of input sequences.\n\n    Returns:\n        vec_pair_list (list): List of tuples containing vectorized input-output pairs.\n        char2index (dict): Updated character to index dictionary.\n        index2char (dict): Updated index to character dictionary.\n    \"\"\"\n    # Adding characters to the main index2char and char2index dictionary\n    for word_pair in data_pairs:\n        # Add characters to the index2char and char2index dictionaries\n        for char in word_pair[0]:\n            if char not in char2index:\n                char2index[char] = len(char2index)\n                index2char[len(index2char)] = char\n\n        for char in word_pair[1]:\n\n            if char not in char2index:\n                char2index[char] = len(char2index)\n                index2char[len(index2char)] = char\n\n    # defining padding character index\n    SOS_token = 0\n    EOS_token = 1\n    PAD_token = 2\n\n    vec_pair_list = []\n    for word_pair in data_pairs:\n        # Vectorize English word\n        eng_vec = [char2index[char] for char in word_pair[0]]\n        eng_vec.append(EOS_token)\n        eng_vec += [PAD_token] * (MAX_LENGTH - len(word_pair[0]))\n        eng_vec = torch.LongTensor(eng_vec)\n\n        # Vectorize Gujarati word\n        guj_vec = [char2index[char] for char in word_pair[1]]\n        guj_vec.append(EOS_token)\n        guj_vec += [PAD_token] * (MAX_LENGTH - len(word_pair[1]))\n        guj_vec = torch.LongTensor(guj_vec)\n\n        vec_pair = (eng_vec, guj_vec)\n        vec_pair_list.append(vec_pair)\n\n    return vec_pair_list, char2index, index2char\n","metadata":{"id":"4hGs3dbCpjNB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Method","metadata":{"id":"0NkjOiDlJM9T"}},{"cell_type":"code","source":"def train(encoder, decoder,train_loader, encoder_optimizer,decoder_optimizer,encoder_num_layers,decoder_num_layers,cell_type,criterion,char2index,index2char,MAX_LENGTH,teacher_forcing_ratio,attention, device):\n    \"\"\"\n    Train the encoder-decoder model.\n\n    Parameters:\n        encoder (nn.Module): Encoder model.\n        decoder (nn.Module): Decoder model.\n        train_loader (DataLoader): DataLoader for training data.\n        encoder_optimizer (torch.optim): Optimizer for encoder.\n        decoder_optimizer (torch.optim): Optimizer for decoder.\n        encoder_num_layers (int): Number of layers in the encoder.\n        decoder_num_layers (int): Number of layers in the decoder.\n        cell_type (str): Type of RNN cell used in the model.\n        criterion (nn.Module): Loss criterion.\n        char2index (dict): Dictionary mapping characters to indices.\n        index2char (dict): Dictionary mapping indices to characters.\n        MAX_LENGTH (int): Maximum length of input sequences.\n        teacher_forcing_ratio (float): Probability of using teacher forcing during training.\n        attention (bool): Whether to use attention mechanism.\n        device (torch.device): Device to run the model on.\n\n    Returns:\n        final_loss (float): Average loss over the training data.\n        accuracy (float): Accuracy of the model.\n        atten_weights (Tensor): Attention weights.\n    \"\"\"\n    # initializing parameters\n    total_loss = 0\n    correct=0\n    total=0\n    encoder.to(device)\n    decoder.to(device)\n    encoder.train()\n    decoder.train()\n    target_length=0\n\n    atten_weights = torch.zeros(1,MAX_LENGTH+1, MAX_LENGTH+1).to(device)\n\n    # Iterate over the training data\n    for data in tqdm(train_loader):\n\n        # Get input and target tensors\n        input_tensor, target_tensor = data\n        input_tensor = input_tensor.to(device)\n        target_tensor=target_tensor.to(device)\n        batch_size = input_tensor.shape[0]\n\n        # Transpose input and target tensors\n        input_tensor=input_tensor.T\n        target_tensor=target_tensor.T\n\n        # Initialize hidden state for encoder\n        encoder_hidden = encoder.initHidden()\n\n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n        input_length = len(input_tensor)\n        target_length = len(target_tensor)\n\n\n        # Forward pass through encoder\n        encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n\n\n        decoder_input = target_tensor[0]\n\n        # Handle different numbers of layers in the encoder and decoder\n        if encoder_num_layers != decoder_num_layers:\n            if encoder_num_layers < decoder_num_layers:\n                remaining_layers = decoder_num_layers - encoder_num_layers\n                # Copy all encoder hidden layers and then repeat the top layer\n                if cell_type == \"LSTM\":\n                    top_layer_hidden = (encoder_hidden[0][-1].unsqueeze(0), encoder_hidden[1][-1].unsqueeze(0))\n                    extra_hidden = (top_layer_hidden[0].repeat(remaining_layers, 1, 1), top_layer_hidden[1].repeat(remaining_layers, 1, 1))\n                    decoder_hidden = (torch.cat((encoder_hidden[0], extra_hidden[0]), dim=0), torch.cat((encoder_hidden[1], extra_hidden[1]), dim=0))\n                else:\n                    top_layer_hidden = encoder_hidden[-1].unsqueeze(0) #top_layer_hidden shape (1, batch_size, hidden_size)\n                    extra_hidden = top_layer_hidden.repeat(remaining_layers, 1, 1)\n                    decoder_hidden = torch.cat((encoder_hidden, extra_hidden), dim=0)\n\n            else:\n                # Slice the hidden states of the encoder to match the decoder layers\n                if cell_type == \"LSTM\":\n                    decoder_hidden = (encoder_hidden[0][-decoder_num_layers:], encoder_hidden[1][-decoder_num_layers:])\n                else :\n                    decoder_hidden = encoder_hidden[-decoder_num_layers:]\n        else:\n            decoder_hidden = encoder_hidden\n\n\n        loss = 0\n        # Initializing atten_weight_default\n        atten_weight_default = torch.zeros(batch_size,1, MAX_LENGTH + 1).to(device)\n\n        # Initializing prediction tensor\n        pred=torch.zeros(len(target_tensor), batch_size).to(device)\n\n        # Checking for teacher forcing\n        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n        if use_teacher_forcing:\n            # Teacher forcing: Feed the target as the next input\n            for di in range(target_length):\n\n                if attention == True:\n                    # calling decoder object\n                    decoder_output, decoder_hidden, atten_weight = decoder(decoder_input, decoder_hidden, encoder_output)\n                    # Concate default weights with weights returned by decoder\n                    atten_weight_default = torch.cat((atten_weight_default, atten_weight), dim = 1)\n\n                else:\n                    # calling decoder object\n                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n\n                # Squeezing decoder output\n                decoder_output = torch.squeeze(decoder_output)\n\n                # Accumulating loss\n                loss += criterion(decoder_output, target_tensor[di].long())\n\n                # Taking top k prediction of decoder\n                topv, topi = decoder_output.topk(1)\n                topi=torch.squeeze(topi)\n\n                pred[di]=topi\n\n\n                # Assigning input to decoder\n                decoder_input = target_tensor[di]\n\n\n        else:\n\n            # Without teacher forcing: use its own predictions as the next input\n            for di in range(target_length):\n\n                if attention == True:\n                    # calling decoder object\n                    decoder_output, decoder_hidden, atten_weight = decoder(decoder_input, decoder_hidden, encoder_output)\n\n                    # Concate default weights with weights returned by decoder\n                    atten_weight_default = torch.cat((atten_weight_default, atten_weight), dim = 1)\n\n                else:\n                    # calling decoder object\n                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n\n\n                # Squeezing decoder output\n                decoder_output = torch.squeeze(decoder_output)\n                # Accumulating loss\n                loss += criterion(decoder_output, target_tensor[di].long())\n\n                # Taking top k prediction of decoder\n                topv, topi = decoder_output.topk(1)\n                topi = torch.squeeze(topi)\n                pred[di]=topi\n\n                # Assigning input to decoder\n                decoder_input = topi\n\n\n        if attention == True:\n            # Concating weights returned by decoder to default weights\n            atten_weights = torch.cat((atten_weights, atten_weight_default[:, 1:, :]), dim = 0)\n\n        # Transpose the prediction and target tensor as first dimention was batch_size\n        pred = pred.T\n        act = target_tensor.T\n\n\n        # Calculate accuracy\n        for i in range(len(pred)):\n            # taking flag variable\n            f=0\n            for j in range(len(pred[i])):\n                # checking is character is not padding character\n                if(pred[i][j]!=act[i][j]):\n\n                    # f=1 to track if one of the character not matched then break the for loop and check for new word\n                    f=1\n                    break\n            # f==0 means all the characters matched to actual word so increase count\n            if(f==0):\n                correct += 1\n\n        # Accumulating loss\n        total_loss += loss\n        loss.backward()\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n\n\n    # calculate accuracy\n    accuracy = correct / (len(train_loader) * batch_size )\n    # calculate loss\n    final_loss = total_loss /  (len(train_loader) * batch_size )\n\n    # returning loss,accuracy and attention weights\n    return   final_loss , accuracy,atten_weights[1:, :, :]\n\n\n","metadata":{"id":"Judy7ffNQ6S9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate Method","metadata":{"id":"Gmjda08Up3XP"}},{"cell_type":"code","source":"\n# Define evaluation function\ndef evaluate(encoder, decoder, val_loader,encoder_num_layers,decoder_num_layers,cell_type, criterion,char2index,index2char,MAX_LENGTH,attention, device):\n    \"\"\"\n    Evaluate the encoder-decoder model.\n\n    Parameters:\n        encoder (nn.Module): Encoder model.\n        decoder (nn.Module): Decoder model.\n        val_loader (DataLoader): DataLoader for validation data.\n        encoder_num_layers (int): Number of layers in the encoder.\n        decoder_num_layers (int): Number of layers in the decoder.\n        cell_type (str): Type of RNN cell used in the model.\n        criterion (nn.Module): Loss criterion.\n        char2index (dict): Dictionary mapping characters to indices.\n        index2char (dict): Dictionary mapping indices to characters.\n        MAX_LENGTH (int): Maximum length of input sequences.\n        attention (bool): Whether to use attention mechanism.\n        device (torch.device): Device to run the model on.\n\n    Returns:\n        final_loss (float): Average loss over the validation data.\n        accuracy (float): Accuracy of the model.\n        predictions (list): Predicted words.\n        Input (list): Input words.\n        Target (list): Target words.\n        atten_weights (Tensor): Attention weights.\n    \"\"\"\n\n\n    # initializing parameters\n    EOS_token=1\n    SOS_token=0\n    correct=0\n    total_loss=0\n    target_length=0\n    pred=[]\n    predictions = []\n    Input = []\n    Target = []\n    atten_weights = torch.zeros(1,MAX_LENGTH+1, MAX_LENGTH+1).to(device)\n\n    with torch.no_grad():\n\n        encoder.eval()\n        decoder.eval()\n        for data in tqdm(val_loader):\n\n            # Extract input and target tensors\n            input_tensor, target_tensor = data\n            batch_size = input_tensor.shape[0]\n            input_tensor = input_tensor.to(device)\n            target_tensor=target_tensor.to(device)\n\n            # Transpose tensors for batch processing\n            input_tensor=input_tensor.T\n            target_tensor=target_tensor.T\n\n            # Initialize encoder hidden state\n            encoder_hidden = encoder.initHidden()\n\n            input_length = len(input_tensor)\n            target_length = len(target_tensor)\n\n            # Forward pass through encoder\n            encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n\n\n            decoder_input = target_tensor[0]\n\n            # Handle different numbers of layers in the encoder and decoder\n            if encoder_num_layers != decoder_num_layers:\n                if encoder_num_layers < decoder_num_layers:\n                    remaining_layers = decoder_num_layers - encoder_num_layers\n                    # Copy all encoder hidden layers and then repeat the top layer\n                    if cell_type == \"LSTM\":\n                        top_layer_hidden = (encoder_hidden[0][-1].unsqueeze(0), encoder_hidden[1][-1].unsqueeze(0))\n                        extra_hidden = (top_layer_hidden[0].repeat(remaining_layers, 1, 1), top_layer_hidden[1].repeat(remaining_layers, 1, 1))\n                        decoder_hidden = (torch.cat((encoder_hidden[0], extra_hidden[0]), dim=0), torch.cat((encoder_hidden[1], extra_hidden[1]), dim=0))\n                    else:\n                        top_layer_hidden = encoder_hidden[-1].unsqueeze(0) #top_layer_hidden shape (1, batch_size, hidden_size)\n                        extra_hidden = top_layer_hidden.repeat(remaining_layers, 1, 1)\n                        decoder_hidden = torch.cat((encoder_hidden, extra_hidden), dim=0)\n\n                else:\n                    # Slice the hidden states of the encoder to match the decoder layers\n                    if cell_type == \"LSTM\":\n                        decoder_hidden = (encoder_hidden[0][-decoder_num_layers:], encoder_hidden[1][-decoder_num_layers:])\n                    else :\n                        decoder_hidden = encoder_hidden[-decoder_num_layers:]\n            else:\n                decoder_hidden = encoder_hidden\n\n\n            loss = 0\n\n            # Initializing prediction tensor\n            pred=torch.zeros(len(target_tensor), batch_size).to(device)\n\n            # Initializing atten_weight_default\n            atten_weight_default = torch.zeros(batch_size,1, MAX_LENGTH + 1).to(device)\n\n\n            # Decode sequence\n            for di in range(target_length):\n                if attention == True:\n\n                    # calling decoder object\n                    decoder_output, decoder_hidden, atten_weight = decoder(decoder_input, decoder_hidden, encoder_output)\n                    # Concate default weights with weights returned by decoder\n                    atten_weight_default = torch.cat((atten_weight_default, atten_weight), dim = 1)\n\n                else:\n                    # calling decoder object\n                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n\n\n                # Squeezing decoder output\n                decoder_output = torch.squeeze(decoder_output)\n\n                # Accumulation loss\n                loss += criterion(decoder_output, target_tensor[di].long())\n\n                # Taking top k prediction of decoder\n                topv, topi = decoder_output.topk(1)\n                topi = torch.squeeze(topi)\n                pred[di]=topi\n\n                # Assigning input to decoder\n                decoder_input = topi\n\n            if attention == True:\n                # Concating weights returned by decoder to default weights\n                atten_weights = torch.cat((atten_weights, atten_weight_default[:, 1:, :]), dim = 0)\n\n            # Transpose the prediction and target tensor as first dimention was batch_size\n            pred = pred.T\n            act = target_tensor.T\n            act_eng = input_tensor.T\n\n            # Collect predictions, inputs, and targets\n            for i in range(batch_size):\n                pred_word=\"\"\n                input_word=\"\"\n                target_word = \"\"\n\n                # Taking flag variable\n                f=0\n                for j in range(len(act[i])):\n\n                    # checking is character is not padding character\n                    if(int(pred[i][j].item()) > 2):\n                        pred_word += index2char[int(pred[i][j].item())]\n\n                    # checking is character is not padding character\n                    if(int(act[i][j].item()) > 2):\n                        target_word += index2char[int(act[i][j].item())]\n\n                    if(pred[i][j]!=act[i][j]):\n                        # f=1 to track if one of the character not matched then break the for loop and check for new word\n                        f=1\n\n                # f==0 means all the characters matched to actual word so increase count\n                if(f==0):\n                    correct += 1\n\n                for j in range(len(act_eng[i])):\n                      # checking is character is not padding character\n                      if(int(act_eng[i][j].item()) > 2):\n                          input_word += index2char[int(act_eng[i][j].item())]\n\n\n                # Appending words to list\n                predictions.append(pred_word)\n                Input.append(input_word)\n                Target.append(target_word)\n\n        # Accumulating loss\n        total_loss += loss\n        # Calculating Accuracy\n        accuracy = correct / (len(val_loader) * batch_size )\n        # Calculating Loss\n        final_loss = total_loss / (len(val_loader) * batch_size )\n\n    # returning loss ,accuracy,prediction list,input list,target list and attention weights\n    return  final_loss , accuracy , predictions ,Input , Target ,atten_weights[1:, :, :]\n","metadata":{"id":"o2hz9Lqt5bR4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Heatmap plot using wandb plots","metadata":{"id":"KI25QHpyp8HU"}},{"cell_type":"code","source":"# For plotting attention using wandb this is required\n!pip install scikit-learn","metadata":{"id":"tE2vHqs-HdOo","outputId":"27efcc12-3e5e-4a42-ce33-7f578d1907c3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_attention(atten_weights,predictions,data_pairs_test):\n    import sklearn\n\n    # Define the grid dimensions\n    rows = int(np.ceil(np.sqrt(10)))\n    cols = int(np.ceil(10 / rows))\n\n\n    for i in range(10):\n        if i < 10:\n            # taking each predicted word basd on index i\n            prediction = predictions[i]\n            # initialize empty strings\n            pred_word=\"\"\n            input_word=\"\"\n\n            for j in range(len(prediction)):\n                # Ignore padding\n                if(prediction[j] != '#'):\n                    # Append the character to the word\n                    pred_word += prediction[j]\n                else :\n                    break\n            # Taking input word from data_pairs_test\n\n            input_seq = data_pairs_test[i][0]\n\n\n\n            for j in range(len(input_seq)):\n                # Ignore padding\n                if(input_seq[j] != '#'):\n                    # Append the character to the word\n                    input_word += input_seq[j]\n                else :\n                    break\n            # Taking important weights for current word\n            attn_weights = atten_weights[i, :len(pred_word), :len(input_word)].detach().cpu().numpy()\n\n\n            # Create and log each heatmap individually\n            x_labels = list(input_word)\n            y_labels = list(pred_word)\n\n            # calling plot function of wandb it requires wandb version 0.14.0 and also sklearn should be imported as this plot function is depreciated in current wandb version\n            heatmap = wandb.plots.HeatMap(\n                x_labels=x_labels,\n                y_labels=y_labels,\n                matrix_values=attn_weights,\n                show_text=False\n            )\n            # Logging heatmap to wandb current run\n            wandb.log({'heatmap_' + str(i + 1): heatmap}, commit=False)\n","metadata":{"id":"Ydu7goCIHdOo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Driver method","metadata":{"id":"rHEFBhZVqEZf"}},{"cell_type":"code","source":"\n# Define training function\ndef arguments(input_embedding_size,encoder_num_layers,decoder_num_layers,hidden_size,cell_type,bidirectional,batch_size,learning_rate,num_epochs,dropout,teacher_forcing_ratio,attention,mode,Folder_path,Folder_name):\n\n\n    \"\"\"\n    Define arguments and run the training or testing mode.\n\n    Parameters:\n        input_embedding_size (int): Size of input embeddings.\n        encoder_num_layers (int): Number of layers in the encoder.\n        decoder_num_layers (int): Number of layers in the decoder.\n        hidden_size (int): Size of the hidden layer.\n        cell_type (str): Type of RNN cell used in the model.\n        bidirectional (bool): Whether the encoder is bidirectional.\n        batch_size (int): Batch size for training.\n        learning_rate (float): Learning rate for optimization.\n        num_epochs (int): Number of training epochs.\n        dropout (float): Dropout probability.\n        teacher_forcing_ratio (float): Ratio of teacher forcing during training.\n        attention (bool): Whether to use attention mechanism.\n        mode (str): Training or Testing mode.\n        pathofzip (str): Path to the zip file containing the dataset.\n        Folder_name (str): Name of the folder containing the dataset.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Retrieve datasets\n    train_dataset ,val_dataset ,test_dataset= Datasetretrival(Folder_path,Folder_name)\n\n    # Convert DataFrame to list of tuples\n    data_pairs_train = [list(row) for row in train_dataset.values]\n    data_pairs_val = [list(row) for row in val_dataset.values]\n    data_pairs_test = [list(row) for row in test_dataset.values]\n\n\n    # Initialize index-char dictionaries\n    index2char = {0:'<', 1: '>', 2 : '.'}\n    char2index = {'<' : 0, '>' : 1, '.' : 2 }\n\n\n    # Define maximum sequence length\n    MAX_LENGTH= 27\n\n    # Vectorize data\n    vec_pair_list_train,char2index, index2char=vecorizeddata(data_pairs_train,index2char, char2index,MAX_LENGTH)\n    vec_pair_list_val,_,_=vecorizeddata(data_pairs_val,index2char, char2index,MAX_LENGTH)\n    vec_pair_list_test,_,_=vecorizeddata(data_pairs_test,index2char, char2index,MAX_LENGTH)\n\n\n\n    # initialize prediction target and Input List\n    predictions = []\n    Input = []\n    Target = []\n\n    # Initialize device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Initialize attention weights\n    atten_weights = torch.zeros(1,MAX_LENGTH+1, MAX_LENGTH+1).to(device)\n\n    # Create DataLoader objects\n    train_loader = DataLoader(vec_pair_list_train, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(vec_pair_list_val, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(vec_pair_list_test, batch_size=batch_size, shuffle=False)\n\n    # Define model\n    encoder = Encoder(len(char2index),input_embedding_size, hidden_size,batch_size, encoder_num_layers, cell_type,bidirectional, dropout).to(device)\n\n    if attention ==True:\n        # Initialize attention decoder\n        decoder = AttentionDecoderRNN(hidden_size, len(char2index), input_embedding_size,  decoder_num_layers,cell_type, dropout, batch_size, MAX_LENGTH+1).to(device)\n    else:\n        # Initialize normal decoder\n        decoder = Decoder(len(char2index),input_embedding_size, hidden_size,batch_size, decoder_num_layers, cell_type, dropout,MAX_LENGTH).to(device)\n\n\n\n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n\n\n    # Training and Validation loop\n    if(mode=='Normal'):\n\n        for epoch in range(num_epochs):\n            # Train the model\n            train_loss, train_accuracy,_ = train(encoder,decoder, train_loader, encoder_optimizer,decoder_optimizer,encoder_num_layers,decoder_num_layers,cell_type, criterion,char2index,index2char,MAX_LENGTH,teacher_forcing_ratio,attention, device)\n\n            # Validation loop\n            val_loss, val_accuracy,predictions ,Input , Target , _ = evaluate(encoder,decoder, val_loader,encoder_num_layers,decoder_num_layers,cell_type, criterion,char2index,index2char,MAX_LENGTH,attention, device)\n\n\n            # Log metrics to Weights & Biases\n            wandb.log({\n                \"Epoch\": epoch + 1,\n                \"Train_Accuracy\": train_accuracy,\n                \"Train_Loss\": train_loss,\n                \"Val_Accuracy\": val_accuracy,\n                \"Val_Loss\": val_loss\n            })\n\n            # Print epoch results\n            print(f\"\\nEpoch {epoch+1}/{num_epochs},\\n Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f},\\n Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\\n\")\n\n\n    # Training and Testing Loop\n    elif(mode=='Test'):\n\n        for epoch in range(num_epochs):\n            # Train the model\n            train_loss, train_accuracy,_ = train(encoder,decoder, train_loader, encoder_optimizer,decoder_optimizer,encoder_num_layers,decoder_num_layers,cell_type, criterion,char2index,index2char,MAX_LENGTH,teacher_forcing_ratio,attention, device)\n\n            # Test the model\n            test_loss, test_accuracy,predictions ,Input , Target ,atten_weights  = evaluate(encoder,decoder, test_loader,encoder_num_layers,decoder_num_layers,cell_type, criterion,char2index,index2char,MAX_LENGTH,attention, device)\n\n            # Log metrics to Weights & Biases\n            wandb.log({\n                \"Epoch\": epoch + 1,\n                \"Train_Accuracy\": train_accuracy,\n                \"Train_Loss\": train_loss,\n                \"Test_Accuracy\": test_accuracy,\n                \"Test_Loss\": test_loss\n            })\n\n            # Print epoch results\n            print(f\"\\nEpoch {epoch+1}/{num_epochs},\\n Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f},\\n Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\\n\")\n        # Plot attention heatmap\n\n        # If you want to see plotting using this method the wandb version should be 0.14.0 and  scikit-learn package should be installed and import sklearn\n\n        # This wandb plot function is depriciated in current version of wandb so be carefull while calling this function\n\n        # plot_attention(atten_weights,predictions,data_pairs_test)\n\n    print(f\"\\n Predictions Generated by current sweep and actual output:\\n\")\n    # Save predictions to a CSV file\n    dataframe = pd.DataFrame({\"INPUT\": Input, \"PREDICTED\": predictions,\"ACTUAL\":Target})\n    # Converting dataframe to\n    dataframe.to_csv(\"predictions.csv\", index=False)\n    data = pd.read_csv(\"predictions.csv\",header=None)\n    # Display csv\n    # If you want to see csv file in output then make sure \"from IPython.display import display\" is implemented\n    display(data)\n\n\n\n\n","metadata":{"id":"Q-ogr2f5Hk8Q","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login()","metadata":{"id":"_uycA2lZ1b22","outputId":"5bb619e1-a5c2-40b8-c0bd-7ab08b22665f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sweep configuration","metadata":{"id":"Rb0TnlzQqJHZ"}},{"cell_type":"code","source":"\n# Define hyperparameters to sweep\nsweep_config = {\n    \"method\": \"bayes\",\n    'name'  : 'Seq2seq Run ',\n    'metric': {'goal': 'maximize', 'name': 'Val_Accuracy'},\n    \"parameters\": {\n        \"input_embedding_size\": {\"values\": [16, 32, 64, 256]},\n        \"encoder_num_layers\": {\"values\": [1, 2, 3]},\n        \"decoder_num_layers\": {\"values\": [1, 2, 3]},\n        \"hidden_size\": {\"values\": [128,256,512,1024]},\n        \"cell_type\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n        \"bidirectional\": {\"values\": [True, False]},\n        \"batch_size\": {\"values\": [32, 64 , 128]},\n        \"learning_rate\": {\"values\": [0.001, 0.0001]},\n        \"num_epochs\": {\"values\": [5,10,15,20]},\n        \"dropout\": {\"values\": [0.2, 0.3]},\n        \"teacher_forcing_ratio\" : {\"values\":[0.5]},\n        \"attention\": {\"values\": [True,False]}\n    }\n}\n\n\n# Initialize wandb sweep\nsweep_id = wandb.sweep(sweep=sweep_config, project=\"DL_Assignment_3_CS23M046\")\n\n\n\n\n","metadata":{"id":"zLWJQje5FQ2s","outputId":"c212e588-893c-4445-d622-b5c755dad8b6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main_1():\n\n    # Initialize wandb\n    with wandb.init() as run:\n        # Access wandb config\n        config = wandb.config\n        # Run name\n        run_name=str(config.cell_type)+\"_embedding_\"+str(config.input_embedding_size)+\"_hidden_size_\"+str(config.hidden_size)+\"_bidirectional_\"+str(config.bidirectional)+\"_Encoder_layers_\"+str(config.encoder_num_layers)+\"_Decoder_layers_\"+str(config.decoder_num_layers)\n        wandb.run.name=run_name\n\n\n        # add path to folder aksharantar_sampled after unzipped\n        Folder_path='/kaggle/input/aksharantar-sampled/aksharantar_sampled'\n\n        # choose folder name as per your language preference\n        Folder_name='guj'\n\n        # Choose 'Normal' if you want to use train data and validation data else choose 'Test' if you want to choose train data and test data\n        mode= 'Normal'\n\n        # Calling Method\n        arguments(config.input_embedding_size,config.encoder_num_layers,config.decoder_num_layers,config.hidden_size,config.cell_type,config.bidirectional,config.batch_size,config.learning_rate,config.num_epochs,config.dropout,config.teacher_forcing_ratio,config.attention,mode,Folder_path,Folder_name)\n\n\n# Run sweep\nwandb.agent(sweep_id, function=main_1, count=1)\n\nwandb.finish()","metadata":{"id":"2FWR7pg3Mf55","outputId":"a3a36f35-b23e-4f67-a13f-8a0940ff449d","trusted":true},"execution_count":null,"outputs":[]}]}