{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c6fac2d7adf14cc1bc3d4a3aa2ebc9fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb798fbb9fed472a8e435a630ed7dbd2",
              "IPY_MODEL_54cd65690acb490eb66243fdb9a42af7"
            ],
            "layout": "IPY_MODEL_bf93eeafa955456ead301bdfac6cf1aa"
          }
        },
        "cb798fbb9fed472a8e435a630ed7dbd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6954e1cc05d433d99ba6467816ee282",
            "placeholder": "​",
            "style": "IPY_MODEL_0343b4c053454d4d93dc07174f809d64",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        },
        "54cd65690acb490eb66243fdb9a42af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_557a520c02854551b9e2b78bdba3c9b5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3c3a62ca7ca480fb44cb17ff9c97bd5",
            "value": 1
          }
        },
        "bf93eeafa955456ead301bdfac6cf1aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6954e1cc05d433d99ba6467816ee282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0343b4c053454d4d93dc07174f809d64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "557a520c02854551b9e2b78bdba3c9b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3c3a62ca7ca480fb44cb17ff9c97bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9oIymAh7aKG",
        "outputId": "007c0f94-9cf5-4a19-ae2a-ad8d0fa592f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.2.0-py2.py3-none-any.whl (281 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.1/281.1 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.2.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "uEqHxH1GE9Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the model architecture\n",
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, input_size,embedding_size, hidden_size,batch_size,encoder_num_layers, cell_type, bidirectional, dropout):\n",
        "#         super(Encoder, self).__init__()\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.batch_size=batch_size\n",
        "#         self.embedding_size=embedding_size\n",
        "#         self.encoder_num_layers=encoder_num_layers\n",
        "#         self.cell_type = cell_type\n",
        "#         self.bidirectional=bidirectional\n",
        "\n",
        "\n",
        "\n",
        "#         if cell_type == \"RNN\":\n",
        "#             self.rnn = nn.RNN(self.embedding_size, self.hidden_size,self.encoder_num_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "#         elif cell_type == \"LSTM\":\n",
        "#             self.rnn = nn.LSTM(self.embedding_size, self.hidden_size, self.encoder_num_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "#         elif cell_type == \"GRU\":\n",
        "#             self.rnn = nn.GRU(self.embedding_size, self.hidden_size, self.encoder_num_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     def forward(self, input_seq, hidden):\n",
        "#         embedded = self.dropout((self.embedding(input_seq.long())).view(-1,self.batch_size, self.embedding_size))\n",
        "#         outputs, hidden = self.rnn(embedded,hidden)\n",
        "\n",
        "\n",
        "#         if self.bidirectional:\n",
        "#             if self.cell_type == \"LSTM\":\n",
        "#                 # Dividing the hidden state into parts for each direction\n",
        "#                 hidden_state = hidden[0].view(2, self.encoder_num_layers, self.batch_size, self.hidden_size)\n",
        "#                 cell_state = hidden[0].view(2, self.encoder_num_layers, self.batch_size, self.hidden_size)\n",
        "\n",
        "#                 # Combining the hidden and cell states by taking their average\n",
        "#                 hidden = (torch.add(hidden_state[0], hidden_state[1]) / 2, torch.add(cell_state[0], cell_state[1]) / 2)\n",
        "#             else:\n",
        "#                 # Dividing the hidden state into parts for each direction\n",
        "#                 hidden = hidden.view(2, self.encoder_num_layers, self.batch_size, self.hidden_size)\n",
        "\n",
        "#                 # Combining the hidden states by taking their average\n",
        "#                 hidden = torch.add(hidden[0], hidden[1]) / 2\n",
        "\n",
        "#             # Splitting the output tensor into parts for each direction\n",
        "#             split_tensor = torch.split(outputs, self.hidden_size, dim=-1)\n",
        "\n",
        "#             # Combining the outputs by taking their average\n",
        "#             outputs = torch.add(split_tensor[0], split_tensor[1]) / 2\n",
        "\n",
        "\n",
        "#         return outputs, hidden\n",
        "\n",
        "\n",
        "\n",
        "#     def initHidden(self):\n",
        "#             num_directions = 2 if self.bidirectional else 1  # For bidirectional, set to 2, otherwise 1\n",
        "#             if self.cell_type == \"LSTM\":\n",
        "#                 return (torch.zeros(self.encoder_num_layers * num_directions, self.batch_size, self.hidden_size, device=device),\n",
        "#                         torch.zeros(self.encoder_num_layers * num_directions, self.batch_size, self.hidden_size, device=device))\n",
        "#             else:\n",
        "#                 return torch.zeros(self.encoder_num_layers * num_directions, self.batch_size, self.hidden_size, device=device)\n",
        "\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, output_size, embedding_size, hidden_size, batch_size, decoder_num_layers, cell_type, dropout, MAX_LENGTH):\n",
        "#         super(Decoder, self).__init__()\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.output_size = output_size\n",
        "#         self.batch_size = batch_size\n",
        "#         self.embedding_size = embedding_size\n",
        "#         self.decoder_num_layers = decoder_num_layers\n",
        "#         self.MAX_LENGTH = MAX_LENGTH\n",
        "#         self.cell_type = cell_type\n",
        "#         self.dropout=dropout\n",
        "\n",
        "#         self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         if cell_type == \"RNN\":\n",
        "#             self.rnn = nn.RNN(embedding_size, hidden_size, self.decoder_num_layers,dropout=dropout)\n",
        "#         elif cell_type == \"LSTM\":\n",
        "#             self.rnn = nn.LSTM(embedding_size, hidden_size, self.decoder_num_layers, dropout=dropout)\n",
        "#         elif cell_type == \"GRU\":\n",
        "#             self.rnn = nn.GRU(embedding_size, hidden_size, self.decoder_num_layers, dropout=dropout)\n",
        "\n",
        "\n",
        "#         self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "#         self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "#     def forward(self, input, hidden):\n",
        "#         output = self.embedding(input.long()).view(-1,self.batch_size, self.embedding_size)\n",
        "\n",
        "#         output = F.relu(output)\n",
        "\n",
        "#         output, hidden = self.rnn(output, hidden)\n",
        "\n",
        "#         output = self.softmax(self.out(output))\n",
        "#         return output, hidden\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0zYZp7DgFMzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder module of the sequence-to-sequence model.\n",
        "\n",
        "    Parameters:\n",
        "        input_size (int): The size of the input vocabulary.\n",
        "        embedding_size (int): The dimension of the embedding layer.\n",
        "        hidden_size (int): The size of the hidden state of the RNN.\n",
        "        batch_size (int): The size of the batches.\n",
        "        encoder_num_layers (int): The number of layers in the encoder.\n",
        "        cell_type (str): The type of RNN cell: \"RNN\", \"LSTM\", or \"GRU\".\n",
        "        bidirectional (bool): Whether to use bidirectional RNN.\n",
        "        dropout (float): Dropout rate.\n",
        "\n",
        "    Inputs:\n",
        "        input_seq (tensor): Input sequence tensor.\n",
        "        hidden (tensor): Initial hidden state tensor.\n",
        "\n",
        "    Outputs:\n",
        "        outputs (tensor): Encoder outputs tensor.\n",
        "        hidden (tensor): Final hidden state tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, batch_size, encoder_num_layers, cell_type, bidirectional, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.encoder_num_layers = encoder_num_layers\n",
        "        self.cell_type = cell_type\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        if cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.encoder_num_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "        elif cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(self.embedding_size, self.hidden_size, self.encoder_num_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "        elif cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(self.embedding_size, self.hidden_size, self.encoder_num_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "\n",
        "    def forward(self, input_seq, hidden):\n",
        "        embedded = self.dropout((self.embedding(input_seq.long())).view(-1, self.batch_size, self.embedding_size))\n",
        "        outputs, hidden = self.rnn(embedded, hidden)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            if self.cell_type == \"LSTM\":\n",
        "                # Dividing the hidden state into parts for each direction\n",
        "                hidden_state = hidden[0].view(2, self.encoder_num_layers, self.batch_size, self.hidden_size)\n",
        "                cell_state = hidden[0].view(2, self.encoder_num_layers, self.batch_size, self.hidden_size)\n",
        "\n",
        "                # Combining the hidden and cell states by taking their average\n",
        "                hidden = (torch.add(hidden_state[0], hidden_state[1]) / 2, torch.add(cell_state[0], cell_state[1]) / 2)\n",
        "            else:\n",
        "                # Dividing the hidden state into parts for each direction\n",
        "                hidden = hidden.view(2, self.encoder_num_layers, self.batch_size, self.hidden_size)\n",
        "\n",
        "                # Combining the hidden states by taking their average\n",
        "                hidden = torch.add(hidden[0], hidden[1]) / 2\n",
        "\n",
        "            # Splitting the output tensor into parts for each direction\n",
        "            split_tensor = torch.split(outputs, self.hidden_size, dim=-1)\n",
        "\n",
        "            # Combining the outputs by taking their average\n",
        "            outputs = torch.add(split_tensor[0], split_tensor[1]) / 2\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        num_directions = 2 if self.bidirectional else 1  # For bidirectional, set to 2, otherwise 1\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            return (torch.zeros(self.encoder_num_layers * num_directions, self.batch_size, self.hidden_size, device=device),\n",
        "                    torch.zeros(self.encoder_num_layers * num_directions, self.batch_size, self.hidden_size, device=device))\n",
        "        else:\n",
        "            return torch.zeros(self.encoder_num_layers * num_directions, self.batch_size, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder module of the sequence-to-sequence model.\n",
        "\n",
        "    Parameters:\n",
        "        output_size (int): The size of the output vocabulary.\n",
        "        embedding_size (int): The dimension of the embedding layer.\n",
        "        hidden_size (int): The size of the hidden state of the RNN.\n",
        "        batch_size (int): The size of the batches.\n",
        "        decoder_num_layers (int): The number of layers in the decoder.\n",
        "        cell_type (str): The type of RNN cell: \"RNN\", \"LSTM\", or \"GRU\".\n",
        "        dropout (float): Dropout rate.\n",
        "        MAX_LENGTH (int): Maximum length of the input sequence.\n",
        "\n",
        "    Inputs:\n",
        "        input (tensor): Input tensor.\n",
        "        hidden (tensor): Hidden state tensor.\n",
        "\n",
        "    Outputs:\n",
        "        output (tensor): Output tensor.\n",
        "        hidden (tensor): Hidden state tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size, embedding_size, hidden_size, batch_size, decoder_num_layers, cell_type, dropout, MAX_LENGTH):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.decoder_num_layers = decoder_num_layers\n",
        "        self.MAX_LENGTH = MAX_LENGTH\n",
        "        self.cell_type = cell_type\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        if cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(embedding_size, hidden_size, self.decoder_num_layers, dropout=dropout)\n",
        "        elif cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(embedding_size, hidden_size, self.decoder_num_layers, dropout=dropout)\n",
        "        elif cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(embedding_size, hidden_size, self.decoder_num_layers, dropout=dropout)\n",
        "\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input.long()).view(-1, self.batch_size, self.embedding_size)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "        output = self.softmax(self.out(output))\n",
        "        return output, hidden\n"
      ],
      "metadata": {
        "id": "zwS29ZN6maPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class AttentionDecoderRNN(nn.Module):\n",
        "#     def __init__(self, hidden_size, output_size, embedding_size, num_layers,\n",
        "#                  cell_type, dropout, batch_size, max_length):\n",
        "#         super(AttentionDecoderRNN, self).__init__()\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.num_layers = num_layers\n",
        "#         self.cell_type = cell_type\n",
        "#         self.batch_size = batch_size\n",
        "#         self.embedding_size = embedding_size\n",
        "#         self.max_length = max_length\n",
        "#         self.dropout = dropout\n",
        "\n",
        "#         self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "#         self.dropout = nn.Dropout(self.dropout)\n",
        "#         self.attention = nn.Linear(hidden_size + embedding_size, self.max_length)\n",
        "#         self.attention_combine = nn.Linear(hidden_size + embedding_size, hidden_size)\n",
        "\n",
        "#         if self.cell_type == \"GRU\":\n",
        "#             self.rnn = nn.GRU(hidden_size, hidden_size, num_layers=num_layers)\n",
        "#         elif self.cell_type == \"LSTM\":\n",
        "#             self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)\n",
        "#         else:\n",
        "#             self.rnn = nn.RNN(hidden_size, hidden_size, num_layers=num_layers)\n",
        "\n",
        "#         self.out = nn.Linear(hidden_size, output_size)\n",
        "#         self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "#     def forward(self, input, hidden, encoder_outputs): #input shape (1, batch_size)\n",
        "#         embedded = self.embedding(input.long()).view(-1, self.batch_size, self.embedding_size)\n",
        "#         # embedded shape (1, batch_size, embedding_size)\n",
        "#         embedded = F.relu(embedded)\n",
        "\n",
        "#         # Compute attention scores\n",
        "#         if self.cell_type == \"LSTM\":\n",
        "#             attn_hidden = torch.mean(hidden[0], dim=0)\n",
        "#         else:\n",
        "#             attn_hidden = torch.mean(hidden, dim = 0)\n",
        "#         attn_scores = self.attention(torch.cat((embedded, attn_hidden.unsqueeze(0)), dim=2)) # attn_scores shape (1, batch_size, max_length)\n",
        "\n",
        "#         attn_weights = F.softmax(attn_scores, dim=-1)  # attn_scores shape (1, 16, 25)\n",
        "\n",
        "\n",
        "#         # Apply attention weights to encoder outputs\n",
        "#         # print(encoder_outputs.transpose(0, 1).shape)\n",
        "#         attn_applied = torch.bmm(attn_weights.transpose(0, 1), encoder_outputs.transpose(0, 1))\n",
        "\n",
        "#         # Combine attention output and embedded input\n",
        "#         combined = torch.cat((embedded, attn_applied.transpose(0, 1)), dim=2)\n",
        "#         combined = self.attention_combine(combined)\n",
        "#         combined = F.relu(combined) # shape (1, batch_size, hidden_size)\n",
        "\n",
        "#         # Run through the RNN\n",
        "#         output, hidden = self.rnn(combined, hidden)\n",
        "#         # output shape: (1, batch_size, hidden_size)\n",
        "\n",
        "#         # Pass through linear layer and softmax activation\n",
        "#         output = self.out(output)  # shape: (1, batch_size, output_size)\n",
        "#         output = self.softmax(output)\n",
        "#         return output, hidden, attn_weights.transpose(0, 1)"
      ],
      "metadata": {
        "id": "D1pdPamLk8R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionDecoderRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention-based Decoder module of the sequence-to-sequence model.\n",
        "\n",
        "    Parameters:\n",
        "        hidden_size (int): The size of the hidden state of the RNN.\n",
        "        output_size (int): The size of the output vocabulary.\n",
        "        embedding_size (int): The dimension of the embedding layer.\n",
        "        num_layers (int): The number of layers in the RNN.\n",
        "        cell_type (str): The type of RNN cell: \"RNN\", \"LSTM\", or \"GRU\".\n",
        "        dropout (float): Dropout rate.\n",
        "        batch_size (int): The size of the batches.\n",
        "        max_length (int): Maximum length of the input sequence.\n",
        "\n",
        "    Inputs:\n",
        "        input (tensor): Input tensor.\n",
        "        hidden (tensor): Hidden state tensor.\n",
        "        encoder_outputs (tensor): Encoder outputs tensor.\n",
        "\n",
        "    Outputs:\n",
        "        output (tensor): Output tensor.\n",
        "        hidden (tensor): Hidden state tensor.\n",
        "        attn_weights (tensor): Attention weights tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, output_size, embedding_size, num_layers,\n",
        "                 cell_type, dropout, batch_size, max_length):\n",
        "        super(AttentionDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.max_length = max_length\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "        self.attention = nn.Linear(hidden_size + embedding_size, self.max_length)\n",
        "        self.attention_combine = nn.Linear(hidden_size + embedding_size, hidden_size)\n",
        "\n",
        "        if self.cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(hidden_size, hidden_size, num_layers=num_layers)\n",
        "        elif self.cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)\n",
        "        else:\n",
        "            self.rnn = nn.RNN(hidden_size, hidden_size, num_layers=num_layers)\n",
        "\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input.long()).view(-1, self.batch_size, self.embedding_size)\n",
        "        embedded = F.relu(embedded)\n",
        "\n",
        "        # Compute attention scores\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            attn_hidden = torch.mean(hidden[0], dim=0)\n",
        "        else:\n",
        "            attn_hidden = torch.mean(hidden, dim=0)\n",
        "        attn_scores = self.attention(torch.cat((embedded, attn_hidden.unsqueeze(0)), dim=2))\n",
        "\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Apply attention weights to encoder outputs\n",
        "        attn_applied = torch.bmm(attn_weights.transpose(0, 1), encoder_outputs.transpose(0, 1))\n",
        "\n",
        "        # Combine attention output and embedded input\n",
        "        combined = torch.cat((embedded, attn_applied.transpose(0, 1)), dim=2)\n",
        "        combined = self.attention_combine(combined)\n",
        "        combined = F.relu(combined)\n",
        "\n",
        "        # Run through the RNN\n",
        "        output, hidden = self.rnn(combined, hidden)\n",
        "\n",
        "        # Pass through linear layer and softmax activation\n",
        "        output = self.out(output)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden, attn_weights.transpose(0, 1)\n"
      ],
      "metadata": {
        "id": "1M5cmEu_mmfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def Datasetretrival(pathofzip, folder_name):\n",
        "#     # Define the path to your zip file\n",
        "#     zip_file_path = pathofzip\n",
        "\n",
        "#     # Extract the contents of the zip file\n",
        "#     with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "#         zip_ref.extractall('extracted_data')\n",
        "\n",
        "#     # Define the path to the extracted data directory\n",
        "#     extracted_data_dir = 'extracted_data/aksharantar_sampled'\n",
        "#     contents = os.listdir(extracted_data_dir)\n",
        "\n",
        "#     # Initialize empty lists for train, test, and validation datasets\n",
        "#     train_datasets = []\n",
        "#     test_datasets = []\n",
        "#     val_datasets = []\n",
        "\n",
        "#  # Load train, test, and validation CSV files from the specified folder\n",
        "#     folder_path = os.path.join(extracted_data_dir, folder_name)\n",
        "\n",
        "#     # List all files in the folder\n",
        "#     folder_files = os.listdir(folder_path)\n",
        "\n",
        "\n",
        "#     # Filter files with the specified folder name as prefix\n",
        "#     foldername_prefix = folder_name + \"_\"\n",
        "#     folder_files_with_prefix = [file for file in folder_files if file.startswith(foldername_prefix)]\n",
        "\n",
        "#     for file in folder_files_with_prefix:\n",
        "#         file_path = os.path.join(folder_path, file)\n",
        "#         if 'train' in file:\n",
        "#             train_datasets.append(pd.read_csv(file_path,header=None))\n",
        "#         elif 'test' in file:\n",
        "#             test_datasets.append(pd.read_csv(file_path,header=None))\n",
        "#         elif 'val' in file:\n",
        "#             val_datasets.append(pd.read_csv(file_path,header=None))\n",
        "\n",
        "#     # Concatenate the loaded dataframes to create single train, test, and validation datasets\n",
        "#     train_dataset = pd.concat(train_datasets, ignore_index=True)\n",
        "#     test_dataset = pd.concat(test_datasets, ignore_index=True)\n",
        "#     val_dataset = pd.concat(val_datasets, ignore_index=True)\n",
        "\n",
        "\n",
        "#     return train_dataset, val_dataset, test_dataset"
      ],
      "metadata": {
        "id": "SbLBJ4bzz1m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Datasetretrival(pathofzip, folder_name):\n",
        "    \"\"\"\n",
        "    Retrieve datasets from a zip file.\n",
        "\n",
        "    Parameters:\n",
        "        pathofzip (str): Path to the zip file containing the datasets.\n",
        "        folder_name (str): Name of the folder containing the datasets.\n",
        "\n",
        "    Returns:\n",
        "        train_dataset (DataFrame): DataFrame containing the training dataset.\n",
        "        val_dataset (DataFrame): DataFrame containing the validation dataset.\n",
        "        test_dataset (DataFrame): DataFrame containing the test dataset.\n",
        "    \"\"\"\n",
        "    # Extract the contents of the zip file\n",
        "    with zipfile.ZipFile(pathofzip, 'r') as zip_ref:\n",
        "        zip_ref.extractall('extracted_data')\n",
        "\n",
        "    # Define the path to the extracted data directory\n",
        "    extracted_data_dir = 'extracted_data/aksharantar_sampled'\n",
        "    folder_path = os.path.join(extracted_data_dir, folder_name)\n",
        "\n",
        "    # Initialize empty lists for train, test, and validation datasets\n",
        "    train_datasets = []\n",
        "    test_datasets = []\n",
        "    val_datasets = []\n",
        "\n",
        "    # List all files in the folder\n",
        "    folder_files = os.listdir(folder_path)\n",
        "\n",
        "    # Filter files with the specified folder name as prefix\n",
        "    foldername_prefix = folder_name + \"_\"\n",
        "    folder_files_with_prefix = [file for file in folder_files if file.startswith(foldername_prefix)]\n",
        "\n",
        "    # Load train, test, and validation CSV files from the specified folder\n",
        "    for file in folder_files_with_prefix:\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        if 'train' in file:\n",
        "            train_datasets.append(pd.read_csv(file_path, header=None))\n",
        "        elif 'test' in file:\n",
        "            test_datasets.append(pd.read_csv(file_path, header=None))\n",
        "        elif 'val' in file:\n",
        "            val_datasets.append(pd.read_csv(file_path, header=None))\n",
        "\n",
        "    # Concatenate the loaded dataframes to create single train, test, and validation datasets\n",
        "    train_dataset = pd.concat(train_datasets, ignore_index=True)\n",
        "    test_dataset = pd.concat(test_datasets, ignore_index=True)\n",
        "    val_dataset = pd.concat(val_datasets, ignore_index=True)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n"
      ],
      "metadata": {
        "id": "Kf_q-DqimwJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def vecorizeddata(data_pairs,index2char, char2index,MAX_LENGTH):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     # Adding in the main index2char and char2index dictionary\n",
        "#     for word_pair in data_pairs:\n",
        "\n",
        "#         for char in word_pair[0]:\n",
        "#             if char not in  char2index:\n",
        "#                 char2index[char] = len(char2index)\n",
        "#                 index2char[len(index2char)] = char\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         for char in word_pair[1]:\n",
        "#             if char not in  char2index:\n",
        "#                 char2index[char] = len(char2index)\n",
        "#                 index2char[len(index2char)] = char\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     SOS_token = 0\n",
        "#     EOS_token = 1\n",
        "#     PAD_token = 2\n",
        "\n",
        "#     vec_pair_list = []\n",
        "#     for word_pair in data_pairs:\n",
        "\n",
        "\n",
        "#         vec_1 = []\n",
        "#         for char in word_pair[0]:\n",
        "#             vec_1.append(char2index[char])\n",
        "\n",
        "#         wordvec_1 = vec_1\n",
        "#         wordvec_1.append(EOS_token)\n",
        "\n",
        "#         for i in range(MAX_LENGTH  - len(word_pair[0])):\n",
        "#             wordvec_1.append(PAD_token)\n",
        "#         wordvec_1 = torch.LongTensor(wordvec_1)\n",
        "#         eng_vec = wordvec_1\n",
        "\n",
        "\n",
        "#         vec_2 = []\n",
        "#         for char in word_pair[1]:\n",
        "#             vec_2.append(char2index[char])\n",
        "\n",
        "#         wordvec_2 = vec_2\n",
        "#         wordvec_2.append(EOS_token)\n",
        "\n",
        "#         for i in range(MAX_LENGTH  - len(word_pair[1])):\n",
        "#             wordvec_2.append(PAD_token)\n",
        "#         wordvec_2 = torch.LongTensor(wordvec_2)\n",
        "#         guj_vec = wordvec_2\n",
        "\n",
        "\n",
        "#         vec_pair = (eng_vec, guj_vec)\n",
        "#         vec_pair_list.append(vec_pair)\n",
        "\n",
        "#     return vec_pair_list,char2index, index2char\n"
      ],
      "metadata": {
        "id": "4hGs3dbCpjNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def vecorizeddata(data_pairs, index2char, char2index, MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Vectorize the input data pairs for training.\n",
        "\n",
        "    Parameters:\n",
        "        data_pairs (list): List of tuples containing pairs of input-output data.\n",
        "        index2char (dict): Dictionary mapping indices to characters.\n",
        "        char2index (dict): Dictionary mapping characters to indices.\n",
        "        MAX_LENGTH (int): Maximum length of input sequences.\n",
        "\n",
        "    Returns:\n",
        "        vec_pair_list (list): List of tuples containing vectorized input-output pairs.\n",
        "        char2index (dict): Updated character to index dictionary.\n",
        "        index2char (dict): Updated index to character dictionary.\n",
        "    \"\"\"\n",
        "    # Adding characters to the main index2char and char2index dictionary\n",
        "    for word_pair in data_pairs:\n",
        "        for char in word_pair[0]:\n",
        "            if char not in char2index:\n",
        "                char2index[char] = len(char2index)\n",
        "                index2char[len(index2char)] = char\n",
        "\n",
        "        for char in word_pair[1]:\n",
        "            if char not in char2index:\n",
        "                char2index[char] = len(char2index)\n",
        "                index2char[len(index2char)] = char\n",
        "\n",
        "    SOS_token = 0\n",
        "    EOS_token = 1\n",
        "    PAD_token = 2\n",
        "\n",
        "    vec_pair_list = []\n",
        "    for word_pair in data_pairs:\n",
        "        # Vectorize English word\n",
        "        eng_vec = [char2index[char] for char in word_pair[0]]\n",
        "        eng_vec.append(EOS_token)\n",
        "        eng_vec += [PAD_token] * (MAX_LENGTH - len(word_pair[0]))\n",
        "        eng_vec = torch.LongTensor(eng_vec)\n",
        "\n",
        "        # Vectorize Gujarati word\n",
        "        guj_vec = [char2index[char] for char in word_pair[1]]\n",
        "        guj_vec.append(EOS_token)\n",
        "        guj_vec += [PAD_token] * (MAX_LENGTH - len(word_pair[1]))\n",
        "        guj_vec = torch.LongTensor(guj_vec)\n",
        "\n",
        "        vec_pair = (eng_vec, guj_vec)\n",
        "        vec_pair_list.append(vec_pair)\n",
        "\n",
        "    return vec_pair_list, char2index, index2char\n"
      ],
      "metadata": {
        "id": "QK2jcdp6m-bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(encoder, decoder,train_loader, encoder_optimizer,decoder_optimizer,encoder_num_layers,decoder_num_layers,cell_type,criterion,char2index,index2char,MAX_LENGTH,teacher_forcing_ratio,attention, device):\n",
        "    \"\"\"\n",
        "    Train the encoder-decoder model.\n",
        "\n",
        "    Parameters:\n",
        "        encoder (nn.Module): Encoder model.\n",
        "        decoder (nn.Module): Decoder model.\n",
        "        train_loader (DataLoader): DataLoader for training data.\n",
        "        encoder_optimizer (torch.optim): Optimizer for encoder.\n",
        "        decoder_optimizer (torch.optim): Optimizer for decoder.\n",
        "        encoder_num_layers (int): Number of layers in the encoder.\n",
        "        decoder_num_layers (int): Number of layers in the decoder.\n",
        "        cell_type (str): Type of RNN cell used in the model.\n",
        "        criterion (nn.Module): Loss criterion.\n",
        "        char2index (dict): Dictionary mapping characters to indices.\n",
        "        index2char (dict): Dictionary mapping indices to characters.\n",
        "        MAX_LENGTH (int): Maximum length of input sequences.\n",
        "        teacher_forcing_ratio (float): Probability of using teacher forcing during training.\n",
        "        attention (bool): Whether to use attention mechanism.\n",
        "        device (torch.device): Device to run the model on.\n",
        "\n",
        "    Returns:\n",
        "        final_loss (float): Average loss over the training data.\n",
        "        accuracy (float): Accuracy of the model.\n",
        "        atten_weights (Tensor): Attention weights.\n",
        "    \"\"\"\n",
        "\n",
        "    total_loss = 0\n",
        "    correct=0\n",
        "    total=0\n",
        "    encoder.to(device)\n",
        "    decoder.to(device)\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    target_length=0\n",
        "    atten_weights = torch.zeros(1,MAX_LENGTH+1, MAX_LENGTH+1).to(device)\n",
        "\n",
        "    # Iterate over the training data\n",
        "    for data in tqdm(train_loader):\n",
        "\n",
        "        # Get input and target tensors\n",
        "        input_tensor, target_tensor = data\n",
        "        input_tensor = input_tensor.to(device)\n",
        "        target_tensor=target_tensor.to(device)\n",
        "        batch_size = input_tensor.shape[0]\n",
        "        input_tensor=input_tensor.T\n",
        "        target_tensor=target_tensor.T\n",
        "\n",
        "        # Initialize hidden state for encoder\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        input_length = len(input_tensor)\n",
        "        target_length = len(target_tensor)\n",
        "\n",
        "\n",
        "        # Forward pass through encoder\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "\n",
        "\n",
        "        decoder_input = target_tensor[0]\n",
        "\n",
        "        # Handle different numbers of layers in the encoder and decoder\n",
        "        if encoder_num_layers != decoder_num_layers:\n",
        "            if encoder_num_layers < decoder_num_layers:\n",
        "                remaining_layers = decoder_num_layers - encoder_num_layers\n",
        "                # Copy all encoder hidden layers and then repeat the top layer\n",
        "                if cell_type == \"LSTM\":\n",
        "                    top_layer_hidden = (encoder_hidden[0][-1].unsqueeze(0), encoder_hidden[1][-1].unsqueeze(0))\n",
        "                    extra_hidden = (top_layer_hidden[0].repeat(remaining_layers, 1, 1), top_layer_hidden[1].repeat(remaining_layers, 1, 1))\n",
        "                    decoder_hidden = (torch.cat((encoder_hidden[0], extra_hidden[0]), dim=0), torch.cat((encoder_hidden[1], extra_hidden[1]), dim=0))\n",
        "                else:\n",
        "                    top_layer_hidden = encoder_hidden[-1].unsqueeze(0) #top_layer_hidden shape (1, batch_size, hidden_size)\n",
        "                    extra_hidden = top_layer_hidden.repeat(remaining_layers, 1, 1)\n",
        "                    decoder_hidden = torch.cat((encoder_hidden, extra_hidden), dim=0)\n",
        "\n",
        "            else:\n",
        "                # Slice the hidden states of the encoder to match the decoder layers\n",
        "                if cell_type == \"LSTM\":\n",
        "                    decoder_hidden = (encoder_hidden[0][-decoder_num_layers:], encoder_hidden[1][-decoder_num_layers:])\n",
        "                else :\n",
        "                    decoder_hidden = encoder_hidden[-decoder_num_layers:]\n",
        "        else:\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        atten_weight_default = torch.zeros(batch_size,1, MAX_LENGTH + 1).to(device)\n",
        "\n",
        "        pred=torch.zeros(len(target_tensor), batch_size).to(device)\n",
        "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "        if use_teacher_forcing:\n",
        "            # Teacher forcing: Feed the target as the next input\n",
        "            for di in range(target_length):\n",
        "\n",
        "                if attention == True:\n",
        "                    decoder_output, decoder_hidden, atten_weight = decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "                    atten_weight_default = torch.cat((atten_weight_default, atten_weight), dim = 1)\n",
        "\n",
        "                else:\n",
        "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "\n",
        "                decoder_output = torch.squeeze(decoder_output)\n",
        "                loss += criterion(decoder_output, target_tensor[di].long())\n",
        "\n",
        "\n",
        "                topv, topi = decoder_output.topk(1)\n",
        "                topi=torch.squeeze(topi)\n",
        "                pred[di]=topi\n",
        "\n",
        "                decoder_input = target_tensor[di]\n",
        "\n",
        "\n",
        "        else:\n",
        "            # Without teacher forcing: use its own predictions as the next input\n",
        "            for di in range(target_length):\n",
        "            for di in range(target_length):\n",
        "\n",
        "                if attention == True:\n",
        "                    decoder_output, decoder_hidden, atten_weight = decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "                    atten_weight_default = torch.cat((atten_weight_default, atten_weight), dim = 1)\n",
        "\n",
        "                else:\n",
        "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "                decoder_output = torch.squeeze(decoder_output)\n",
        "                loss += criterion(decoder_output, target_tensor[di].long())\n",
        "\n",
        "\n",
        "                topv, topi = decoder_output.topk(1)\n",
        "                topi = torch.squeeze(topi)\n",
        "                pred[di]=topi\n",
        "                decoder_input = topi\n",
        "\n",
        "\n",
        "        if attention == True:\n",
        "            atten_weights = torch.cat((atten_weights, atten_weight_default[:, 1:, :]), dim = 0)\n",
        "\n",
        "        pred = pred.T\n",
        "        act = target_tensor.T\n",
        "\n",
        "\n",
        "        # Calculate accuracy\n",
        "        for i in range(len(pred)):\n",
        "            f=0\n",
        "            for j in range(len(pred[i])):\n",
        "                if(pred[i][j]!=act[i][j]):\n",
        "                    f=1\n",
        "                    break\n",
        "            if(f==0):\n",
        "                correct += 1\n",
        "\n",
        "        total_loss += loss\n",
        "        loss.backward()\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    accuracy = correct / (len(train_loader) * batch_size )\n",
        "    final_loss = total_loss /  (len(train_loader) * batch_size )\n",
        "\n",
        "\n",
        "    return   final_loss , accuracy,atten_weights[1:, :, :]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Judy7ffNQ6S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define evaluation function\n",
        "def evaluate(encoder, decoder, val_loader,encoder_num_layers,decoder_num_layers,cell_type, criterion,char2index,index2char,MAX_LENGTH,attention, device):\n",
        "    \"\"\"\n",
        "    Evaluate the encoder-decoder model.\n",
        "\n",
        "    Parameters:\n",
        "        encoder (nn.Module): Encoder model.\n",
        "        decoder (nn.Module): Decoder model.\n",
        "        val_loader (DataLoader): DataLoader for validation data.\n",
        "        encoder_num_layers (int): Number of layers in the encoder.\n",
        "        decoder_num_layers (int): Number of layers in the decoder.\n",
        "        cell_type (str): Type of RNN cell used in the model.\n",
        "        criterion (nn.Module): Loss criterion.\n",
        "        char2index (dict): Dictionary mapping characters to indices.\n",
        "        index2char (dict): Dictionary mapping indices to characters.\n",
        "        MAX_LENGTH (int): Maximum length of input sequences.\n",
        "        attention (bool): Whether to use attention mechanism.\n",
        "        device (torch.device): Device to run the model on.\n",
        "\n",
        "    Returns:\n",
        "        final_loss (float): Average loss over the validation data.\n",
        "        accuracy (float): Accuracy of the model.\n",
        "        predictions (list): Predicted words.\n",
        "        Input (list): Input words.\n",
        "        Target (list): Target words.\n",
        "        atten_weights (Tensor): Attention weights.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    EOS_token=1\n",
        "    SOS_token=0\n",
        "    correct=0\n",
        "    total_loss=0\n",
        "    target_length=0\n",
        "    pred=[]\n",
        "    predictions = []\n",
        "    Input = []\n",
        "    Target = []\n",
        "    atten_weights = torch.zeros(1,MAX_LENGTH+1, MAX_LENGTH+1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        encoder.eval()\n",
        "        decoder.eval()\n",
        "        for data in tqdm(val_loader):\n",
        "\n",
        "            # Extract input and target tensors\n",
        "            input_tensor, target_tensor = data\n",
        "            batch_size = input_tensor.shape[0]\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            target_tensor=target_tensor.to(device)\n",
        "\n",
        "            # Transpose tensors for batch processing\n",
        "            input_tensor=input_tensor.T\n",
        "            target_tensor=target_tensor.T\n",
        "\n",
        "            # Initialize encoder hidden state\n",
        "            encoder_hidden = encoder.initHidden()\n",
        "\n",
        "            input_length = len(input_tensor)\n",
        "            target_length = len(target_tensor)\n",
        "\n",
        "            # Forward pass through encoder\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "\n",
        "\n",
        "            decoder_input = target_tensor[0]\n",
        "\n",
        "            # Handle different numbers of layers in the encoder and decoder\n",
        "            if encoder_num_layers != decoder_num_layers:\n",
        "                if encoder_num_layers < decoder_num_layers:\n",
        "                    remaining_layers = decoder_num_layers - encoder_num_layers\n",
        "                    # Copy all encoder hidden layers and then repeat the top layer\n",
        "                    if cell_type == \"LSTM\":\n",
        "                        top_layer_hidden = (encoder_hidden[0][-1].unsqueeze(0), encoder_hidden[1][-1].unsqueeze(0))\n",
        "                        extra_hidden = (top_layer_hidden[0].repeat(remaining_layers, 1, 1), top_layer_hidden[1].repeat(remaining_layers, 1, 1))\n",
        "                        decoder_hidden = (torch.cat((encoder_hidden[0], extra_hidden[0]), dim=0), torch.cat((encoder_hidden[1], extra_hidden[1]), dim=0))\n",
        "                    else:\n",
        "                        top_layer_hidden = encoder_hidden[-1].unsqueeze(0) #top_layer_hidden shape (1, batch_size, hidden_size)\n",
        "                        extra_hidden = top_layer_hidden.repeat(remaining_layers, 1, 1)\n",
        "                        decoder_hidden = torch.cat((encoder_hidden, extra_hidden), dim=0)\n",
        "\n",
        "                else:\n",
        "                    # Slice the hidden states of the encoder to match the decoder layers\n",
        "                    if cell_type == \"LSTM\":\n",
        "                        decoder_hidden = (encoder_hidden[0][-decoder_num_layers:], encoder_hidden[1][-decoder_num_layers:])\n",
        "                    else :\n",
        "                        decoder_hidden = encoder_hidden[-decoder_num_layers:]\n",
        "            else:\n",
        "                decoder_hidden = encoder_hidden\n",
        "\n",
        "\n",
        "            loss = 0\n",
        "            pred=torch.zeros(len(target_tensor), batch_size).to(device)\n",
        "            atten_weight_default = torch.zeros(batch_size,1, MAX_LENGTH + 1).to(device)\n",
        "\n",
        "\n",
        "            # Decode sequence\n",
        "            for di in range(target_length):\n",
        "                if attention == True:\n",
        "\n",
        "                    decoder_output, decoder_hidden, atten_weight = decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "                    atten_weight_default = torch.cat((atten_weight_default, atten_weight), dim = 1)\n",
        "\n",
        "                else:\n",
        "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "                decoder_output = torch.squeeze(decoder_output)\n",
        "                loss += criterion(decoder_output, target_tensor[di].long())\n",
        "\n",
        "\n",
        "                topv, topi = decoder_output.topk(1)\n",
        "                topi = torch.squeeze(topi)\n",
        "                pred[di]=topi\n",
        "                decoder_input = topi\n",
        "\n",
        "            if attention == True:\n",
        "                atten_weights = torch.cat((atten_weights, atten_weight_default[:, 1:, :]), dim = 0)\n",
        "\n",
        "            pred = pred.T\n",
        "            act = target_tensor.T\n",
        "            act_eng = input_tensor.T\n",
        "\n",
        "            # Collect predictions, inputs, and targets\n",
        "            for i in range(batch_size):\n",
        "                pred_word=\"\"\n",
        "                input_word=\"\"\n",
        "                target_word = \"\"\n",
        "\n",
        "                f=0\n",
        "                for j in range(len(act[i])):\n",
        "\n",
        "                    if(int(pred[i][j].item()) > 2):\n",
        "                        pred_word += index2char[int(pred[i][j].item())]\n",
        "                    if(int(act[i][j].item()) > 2):\n",
        "                        target_word += index2char[int(act[i][j].item())]\n",
        "\n",
        "                    if(pred[i][j]!=act[i][j]):\n",
        "                        f=1\n",
        "\n",
        "                if(f==0):\n",
        "                    correct += 1\n",
        "\n",
        "                for j in range(len(act_eng[i])):\n",
        "\n",
        "                      if(int(act_eng[i][j].item()) > 2):\n",
        "                          input_word += index2char[int(act_eng[i][j].item())]\n",
        "\n",
        "                predictions.append(pred_word)\n",
        "                Input.append(input_word)\n",
        "                Target.append(target_word)\n",
        "\n",
        "\n",
        "        total_loss += loss\n",
        "        accuracy = correct / (len(val_loader) * batch_size )\n",
        "        final_loss = total_loss / (len(val_loader) * batch_size )\n",
        "    return  final_loss , accuracy , predictions ,Input , Target ,atten_weights[1:, :, :]\n"
      ],
      "metadata": {
        "id": "o2hz9Lqt5bR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define training function\n",
        "def arguments(input_embedding_size,encoder_num_layers,decoder_num_layers,hidden_size,cell_type,bidirectional,batch_size,learning_rate,num_epochs,dropout,teacher_forcing_ratio,attention,mode,pathofzip,Folder_name):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Define arguments and run the training or testing mode.\n",
        "\n",
        "    Parameters:\n",
        "        input_embedding_size (int): Size of input embeddings.\n",
        "        encoder_num_layers (int): Number of layers in the encoder.\n",
        "        decoder_num_layers (int): Number of layers in the decoder.\n",
        "        hidden_size (int): Size of the hidden layer.\n",
        "        cell_type (str): Type of RNN cell used in the model.\n",
        "        bidirectional (bool): Whether the encoder is bidirectional.\n",
        "        batch_size (int): Batch size for training.\n",
        "        learning_rate (float): Learning rate for optimization.\n",
        "        num_epochs (int): Number of training epochs.\n",
        "        dropout (float): Dropout probability.\n",
        "        teacher_forcing_ratio (float): Ratio of teacher forcing during training.\n",
        "        attention (bool): Whether to use attention mechanism.\n",
        "        mode (str): Training or Testing mode.\n",
        "        pathofzip (str): Path to the zip file containing the dataset.\n",
        "        Folder_name (str): Name of the folder containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve datasets\n",
        "    train_dataset ,val_dataset ,test_dataset= Datasetretrival(pathofzip,Folder_name)\n",
        "\n",
        "    # Convert DataFrame to list of tuples\n",
        "    data_pairs_train = [list(row) for row in train_dataset.values]\n",
        "    data_pairs_val = [list(row) for row in val_dataset.values]\n",
        "    data_pairs_test = [list(row) for row in test_dataset.values]\n",
        "\n",
        "\n",
        "    # Initialize index-char dictionaries\n",
        "    index2char = {0:'<', 1: '>', 2 : '.'}\n",
        "    char2index = {'<' : 0, '>' : 1, '.' : 2 }\n",
        "\n",
        "\n",
        "    # Define maximum sequence length\n",
        "    MAX_LENGTH= 27\n",
        "\n",
        "    # Vectorize data\n",
        "    vec_pair_list_train,char2index, index2char=vecorizeddata(data_pairs_train,index2char, char2index,MAX_LENGTH)\n",
        "    vec_pair_list_val,_,_=vecorizeddata(data_pairs_val,index2char, char2index,MAX_LENGTH)\n",
        "    vec_pair_list_test,_,_=vecorizeddata(data_pairs_test,index2char, char2index,MAX_LENGTH)\n",
        "\n",
        "\n",
        "\n",
        "    # initialize prediction target and Input List\n",
        "    predictions = []\n",
        "    Input = []\n",
        "    Target = []\n",
        "\n",
        "    # Initialize device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    atten_weights = torch.zeros(1,MAX_LENGTH+1, MAX_LENGTH+1).to(device)\n",
        "\n",
        "    # Create DataLoader objects\n",
        "    train_loader = DataLoader(vec_pair_list_train, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(vec_pair_list_val, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(vec_pair_list_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Define model\n",
        "    encoder = Encoder(len(char2index),input_embedding_size, hidden_size,batch_size, encoder_num_layers, cell_type,bidirectional, dropout).to(device)\n",
        "\n",
        "    if attention ==True:\n",
        "        decoder = AttentionDecoderRNN(hidden_size, len(char2index), input_embedding_size,  decoder_num_layers,cell_type, dropout, batch_size, MAX_LENGTH+1).to(device)\n",
        "    else:\n",
        "        decoder = Decoder(len(char2index),input_embedding_size, hidden_size,batch_size, decoder_num_layers, cell_type, dropout,MAX_LENGTH).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    #optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    # Training and Validation loop\n",
        "    if(mode=='Normal'):\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Train the model\n",
        "            train_loss, train_accuracy,_ = train(encoder,decoder, train_loader, encoder_optimizer,decoder_optimizer,encoder_num_layers,decoder_num_layers,cell_type, criterion,char2index,index2char,MAX_LENGTH,teacher_forcing_ratio,attention, device)\n",
        "\n",
        "            # Validation loop\n",
        "            val_loss, val_accuracy,predictions ,Input , Target , _ = evaluate(encoder,decoder, val_loader,encoder_num_layers,decoder_num_layers,cell_type, criterion,char2index,index2char,MAX_LENGTH,attention, device)\n",
        "\n",
        "            # # Log metrics to Weights & Biases\n",
        "            # wandb.log({\n",
        "            #     \"Epoch\": epoch + 1,\n",
        "            #     \"Train_Accuracy\": train_accuracy,\n",
        "            #     \"Train_Loss\": train_loss,\n",
        "            #     \"Val_Accuracy\": val_accuracy,\n",
        "            #     \"Val_Loss\": val_loss\n",
        "            # })\n",
        "\n",
        "            # Print epoch results\n",
        "            print(f\"\\nEpoch {epoch+1}/{num_epochs},\\n Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f},\\n Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\\n\")\n",
        "\n",
        "\n",
        "    # Training and Testing Loop\n",
        "    elif(mode=='Test'):\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Train the model\n",
        "            train_loss, train_accuracy,_ = train(encoder,decoder, train_loader, encoder_optimizer,decoder_optimizer,encoder_num_layers,decoder_num_layers,cell_type, criterion,char2index,index2char,MAX_LENGTH,teacher_forcing_ratio,attention, device)\n",
        "\n",
        "            # Test the model\n",
        "            test_loss, test_accuracy,predictions ,Input , Target ,atten_weights  = evaluate(encoder,decoder, test_loader,encoder_num_layers,decoder_num_layers,cell_type, criterion,char2index,index2char,MAX_LENGTH,attention, device)\n",
        "\n",
        "            # # Log metrics to Weights & Biases\n",
        "            # wandb.log({\n",
        "            #     \"Epoch\": epoch + 1,\n",
        "            #     \"Train_Accuracy\": train_accuracy,\n",
        "            #     \"Train_Loss\": train_loss,\n",
        "            #     \"Val_Accuracy\": val_accuracy,\n",
        "            #     \"Val_Loss\": val_loss\n",
        "            # })\n",
        "\n",
        "            # Print epoch results\n",
        "            print(f\"\\nEpoch {epoch+1}/{num_epochs},\\n Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f},\\n Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\\n\")\n",
        "\n",
        "\n",
        "        # Initialize wandb\n",
        "        wandb.init(project=\"DL_Assignment_3_CS23M046\", name=\"attn_visualization\")\n",
        "\n",
        "        # Define the grid dimensions\n",
        "        rows = int(np.ceil(np.sqrt(10)))\n",
        "        cols = int(np.ceil(10 / rows))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(10):\n",
        "            if i < 10:\n",
        "                prediction = predictions[i]\n",
        "\n",
        "                pred_word=\"\"\n",
        "                input_word=\"\"\n",
        "\n",
        "                for j in range(len(prediction)):\n",
        "                    # Ignore padding\n",
        "                    if(prediction[j] != '#'):\n",
        "                        pred_word += prediction[j]\n",
        "                    else :\n",
        "                        break\n",
        "                input_seq = data_pairs_test[i][0]\n",
        "\n",
        "\n",
        "\n",
        "                for j in range(len(input_seq)):\n",
        "                    if(input_seq[j] != '#'):\n",
        "                        input_word += input_seq[j]\n",
        "                    else :\n",
        "                        break\n",
        "                attn_weights = atten_weights[i, :len(pred_word), :len(input_word)].detach().cpu().numpy()\n",
        "\n",
        "                # Create and log each heatmap individually\n",
        "                x_labels = list(input_word)\n",
        "                y_labels = list(pred_word)\n",
        "                heatmap = wandb.plots.HeatMap(\n",
        "                    x_labels=x_labels,\n",
        "                    y_labels=y_labels,\n",
        "                    matrix_values=attn_weights,\n",
        "                    show_text=True\n",
        "                )\n",
        "                wandb.log({'heatmap_' + str(i + 1): heatmap}, commit=False)\n",
        "\n",
        "\n",
        "        # Finish the wandb run\n",
        "        wandb.finish()\n",
        "\n",
        "    print(f\"\\n Predictions Generated by current sweep and actual output:\\n\")\n",
        "    dataframe = pd.DataFrame({\"INPUT\": Input, \"PREDICTED\": predictions,\"ACTUAL\":Target})\n",
        "    dataframe.to_csv(\"predictions.csv\", index=False)\n",
        "    data = pd.read_csv(\"predictions.csv\",header=None)\n",
        "    #files.download(\"predictions.csv\")\n",
        "    display(data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q-ogr2f5Hk8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arguments(32,2,2,1024,\"LSTM\",True,128,0.001,2,0.2,0.5,True,'Test','/content/sample_data/aksharantar_sampled.zip','guj')"
      ],
      "metadata": {
        "id": "AdPJ4R11AYdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "_uycA2lZ1b22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb619e1-a5c2-40b8-c0bd-7ab08b22665f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define hyperparameters to sweep\n",
        "sweep_config = {\n",
        "    \"method\": \"bayes\",\n",
        "    'name'  : 'Train Dataset Run',\n",
        "    'metric': {'goal': 'maximize', 'name': 'Val_Accuracy'},\n",
        "    \"parameters\": {\n",
        "        \"input_embedding_size\": {\"values\": [16, 32, 64, 256]},\n",
        "        \"encoder_num_layers\": {\"values\": [1, 2, 3]},\n",
        "        \"decoder_num_layers\": {\"values\": [1, 2, 3]},\n",
        "        \"hidden_size\": {\"values\": [128,256,512,1024]},\n",
        "        \"cell_type\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n",
        "        \"bidirectional\": {\"values\": [True, False]},\n",
        "        \"batch_size\": {\"values\": [32, 64 , 128]},\n",
        "        \"learning_rate\": {\"values\": [0.001, 0.0001]},\n",
        "        \"num_epochs\": {\"values\": [5,10 ,15]},\n",
        "        \"dropout\": {\"values\": [0.2, 0.3]},\n",
        "        \"teacher_forcing_ratio\" : {\"values\":[0.5]},\n",
        "        \"attention\": {\"values\": [False]},\n",
        "        \"mode\": {\"values\": [\"Normal\",\"Test\"]}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize wandb sweep\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project=\"DL_Assignment_3_CS23M046\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zLWJQje5FQ2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c212e588-893c-4445-d622-b5c755dad8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: mtokz2ko\n",
            "Sweep URL: https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046/sweeps/mtokz2ko\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main_1():\n",
        "\n",
        "    # Initialize wandb\n",
        "    with wandb.init() as run:\n",
        "\n",
        "        config = wandb.config\n",
        "\n",
        "        run_name=str(config.cell_type)+\"_embedding_\"+str(config.input_embedding_size)+\"_hidden_size_\"+str(config.hidden_size)+\"_bidirectional_\"+str(config.bidirectional)+\"_Encoder_layers_\"+str(config.encoder_num_layers)+\"_Decoder_layers_\"+str(config.decoder_num_layers)\n",
        "        wandb.run.name=run_name\n",
        "\n",
        "        pathofzip='/content/sample_data/aksharantar_sampled.zip'\n",
        "        Folder_name='guj'\n",
        "\n",
        "        arguments(config.input_embedding_size,config.encoder_num_layers,config.decoder_num_layers,config.hidden_size,config.cell_type,config.bidirectional,config.batch_size,config.learning_rate,config.num_epochs,config.dropout,config.teacher_forcing_ratio,config.attention,config.mode,pathofzip,Folder_name)\n",
        "\n",
        "\n",
        "\n",
        "# Run sweep\n",
        "wandb.agent(sweep_id, function=main_1, count=10)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "2FWR7pg3Mf55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486,
          "referenced_widgets": [
            "c6fac2d7adf14cc1bc3d4a3aa2ebc9fe",
            "cb798fbb9fed472a8e435a630ed7dbd2",
            "54cd65690acb490eb66243fdb9a42af7",
            "bf93eeafa955456ead301bdfac6cf1aa",
            "e6954e1cc05d433d99ba6467816ee282",
            "0343b4c053454d4d93dc07174f809d64",
            "557a520c02854551b9e2b78bdba3c9b5",
            "f3c3a62ca7ca480fb44cb17ff9c97bd5"
          ]
        },
        "outputId": "a3a36f35-b23e-4f67-a13f-8a0940ff449d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lrfylk5g with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_num_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_num_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 1024\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_embedding_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240508_121441-lrfylk5g</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046/runs/lrfylk5g' target=\"_blank\">absurd-sweep-1</a></strong> to <a href='https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046/sweeps/mtokz2ko' target=\"_blank\">https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046/sweeps/mtokz2ko</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046' target=\"_blank\">https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046/sweeps/mtokz2ko' target=\"_blank\">https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046/sweeps/mtokz2ko</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046/runs/lrfylk5g' target=\"_blank\">https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046/runs/lrfylk5g</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 137/400 [01:06<02:11,  2.00it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
            " 35%|███▌      | 141/400 [01:08<02:09,  1.99it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6fac2d7adf14cc1bc3d4a3aa2ebc9fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▋      | 145/400 [01:10<02:08,  1.99it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">absurd-sweep-1</strong> at: <a href='https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046/runs/lrfylk5g' target=\"_blank\">https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046/runs/lrfylk5g</a><br/> View project at: <a href='https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046' target=\"_blank\">https://wandb.ai/cs23m046/DL_Assignment_3_CS23M046</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240508_121441-lrfylk5g/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "DEFAULT_HYPERPARAMETERS = {\n",
        "\n",
        "    'wandb_project': 'DL_Assignment_2_CS23M046',\n",
        "    'wandb_entity': 'cs23m046',\n",
        "    'Folder_path': None,\n",
        "    'Folder_name' : None ,\n",
        "    'input_embedding_size': [256],\n",
        "    'encoder_num_layers':  [3],\n",
        "    'decoder_num_layers': [3],\n",
        "    'hidden_size':  [1024],\n",
        "    'cell_type':  ['LSTM'],\n",
        "    'bidirectional': [True],\n",
        "    'batch_size': {'values': [128]},\n",
        "    'learning_rate': {'values': [0.0001]},\n",
        "    'num_epochs': {'values': [15]},\n",
        "    'dropout':  [0.2],\n",
        "    'teacher_forcing_ratio': [0.5],\n",
        "    'attention': [True],\n",
        "    'mode':  ['Normal']\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"RNN based seq2seq model\")\n",
        "\n",
        "    parser.add_argument(\"--wandb_project\", type=str, default=DEFAULT_HYPERPARAMETERS['wandb_project'], help=\"Project name used to track experiments in Weights & Biases dashboard\")\n",
        "    parser.add_argument(\"--wandb_entity\", type=str, default=DEFAULT_HYPERPARAMETERS['wandb_entity'], help=\"Wandb Entity used to track experiments in the Weights & Biases dashboard.\")\n",
        "    parser.add_argument(\"--Folder_path\", type=str, default=DEFAULT_HYPERPARAMETERS['Folder_path'], help=\" Folder Path\")\n",
        "    parser.add_argument(\"--Folder_name\", type=str, default=DEFAULT_HYPERPARAMETERS['Folder_name'], help=\"Folder name of Language\")\n",
        "    parser.add_argument(\"--input_embedding_size\", type=int, nargs='+', default=DEFAULT_HYPERPARAMETERS['input_embedding_size'], help=\"Input embedding size\")\n",
        "    parser.add_argument(\"--encoder_num_layers\", type=int, nargs='+', default=DEFAULT_HYPERPARAMETERS['encoder_num_layers'], help=\"Number of layers in the encoder\")\n",
        "    parser.add_argument(\"--decoder_num_layers\", type=int, nargs='+', default=DEFAULT_HYPERPARAMETERS['decoder_num_layers'], help=\"Number of layers in the decoder\")\n",
        "    parser.add_argument(\"--hidden_size\", type=int, nargs='+', default=DEFAULT_HYPERPARAMETERS['hidden_size'], help=\"Hidden size of the model\")\n",
        "    parser.add_argument(\"--cell_type\", type=str, nargs='+', default=DEFAULT_HYPERPARAMETERS['cell_type'], help=\"Type of cell used in the model\")\n",
        "    parser.add_argument(\"--bidirectional\", type=bool, nargs='+', default=DEFAULT_HYPERPARAMETERS['bidirectional'], help=\"Whether to use bidirectional RNN\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, nargs='+', default=DEFAULT_HYPERPARAMETERS['batch_size'], help=\"Batch size for training data\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, nargs='+', default=DEFAULT_HYPERPARAMETERS['learning_rate'], help=\"Learning rate for model training\")\n",
        "    parser.add_argument(\"--num_epochs\", type=int, nargs='+', default=DEFAULT_HYPERPARAMETERS['num_epochs'], help=\"Number of epochs for training\")\n",
        "    parser.add_argument(\"--dropout\", type=float, nargs='+', default=DEFAULT_HYPERPARAMETERS['dropout'], help=\"Dropout rate to prevent overfitting\")\n",
        "    parser.add_argument(\"--teacher_forcing_ratio\", type=float, nargs='+', default=DEFAULT_HYPERPARAMETERS['teacher_forcing_ratio'], help=\"Teacher forcing ratio\")\n",
        "    parser.add_argument(\"--attention\", type=bool, nargs='+', default=DEFAULT_HYPERPARAMETERS['attention'], help=\"Whether to use attention mechanism\")\n",
        "    parser.add_argument(\"--mode\", type=str, default=DEFAULT_HYPERPARAMETERS['mode'], help=\"Mode of operation\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "    # Extracting individual hyperparameters\n",
        "    wandb_project = args.wandb_project\n",
        "    wandb_entity = args.wandb_entity\n",
        "    Folder_path = args.Folder_path\n",
        "    Folder_name = args.Folder_name\n",
        "    input_embedding_size = args.input_embedding_size[0]\n",
        "    encoder_num_layers = args.encoder_num_layers[0]\n",
        "    decoder_num_layers = args.decoder_num_layers[0]\n",
        "    hidden_size = args.hidden_size[0]\n",
        "    cell_type = args.cell_type[0]\n",
        "    bidirectional = args.bidirectional[0]\n",
        "    batch_size = args.batch_size[0]\n",
        "    learning_rate = args.learning_rate[0]\n",
        "    num_epochs = args.num_epochs[0]\n",
        "    dropout = args.dropout[0]\n",
        "    teacher_forcing_ratio = args.teacher_forcing_ratio[0]\n",
        "    attention = args.attention[0]\n",
        "    mode = args.mode\n",
        "\n",
        "\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=wandb_project, entity=wandb_entity)\n",
        "    run_name=str(cell_type)+\"_embedding_\"+str(input_embedding_size)+\"_hidden_size_\"+str(hidden_size)+\"_bidirectional_\"+str(bidirectional)+\"_Encoder_layers_\"+str(encoder_num_layers)+\"_Decoder_layers_\"+str(decoder_num_layers)\n",
        "    wandb.run.name = run_name\n",
        "\n",
        "\n",
        "    arguments(input_embedding_size,encoder_num_layers,decoder_num_layers,hidden_size,cell_type,bidirectional,batch_size,learning_rate,num_epochs,dropout,teacher_forcing_ratio,attention,mode,Folder_path,Folder_name)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ZTgENPMMq2_A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}